{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "# !pip install -qU langchain # Uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the stuff in this notebook from https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb\n",
    "\n",
    "# Main flow:\n",
    "# Install dependencies\n",
    "# Load env variables\n",
    "# Create LLM\n",
    "# Create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get env variables\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY) # Check if API key loaded successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llm (non-chat)\n",
    "from langchain.llms import OpenAI # This is only for non-chat models\n",
    "\n",
    "# For a list of OpenAi models see: https://platform.openai.com/docs/models/overview\n",
    "model_name=\"text-davinci-003\"\n",
    "\n",
    "#Other model parameters\n",
    "temperature = 0.5\n",
    "\n",
    "# non-chat model, eg \"text-davinci-003\"\n",
    "openai_llm = OpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of a prompt\n",
    "# https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/01-langchain-prompt-templates.ipynb\n",
    "# Instructions\n",
    "# External information or content\n",
    "# User input or query\n",
    "# Output indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt technique 1: Basic prompt\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "basic_prompt_template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "basic_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=basic_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user query\n",
    "query = \"Which libraries and model providers offer LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the user's query into the prompt\n",
    "print(basic_prompt_template.format(query=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt technique 2: Few shot prompt\n",
    "from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Create examples\n",
    "examples = [\n",
    "     {\n",
    "        \"question\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"question\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"question\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"question\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"question\": \"What type of artificial intelligence do you use to handle complex tasks?\",\n",
    "        \"answer\": \"I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\"\n",
    "    }, {\n",
    "        \"question\": \"What is your favorite color?\",\n",
    "        \"answer\": \"79\"\n",
    "    }, {\n",
    "        \"question\": \"What is your favorite food?\",\n",
    "        \"answer\": \"Carbon based lifeforms\"\n",
    "    }, {\n",
    "        \"question\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"question\": \"What is the best thing in the world?\",\n",
    "        \"answer\": \"The perfect pizza.\"\n",
    "    }, {\n",
    "        \"question\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"question\": \"If you could do anything in the world what would you do?\",\n",
    "        \"answer\": \"Take over the world, of course!\"\n",
    "    }, {\n",
    "        \"question\": \"Where should I travel?\",\n",
    "        \"answer\": \"If you're looking for adventure, try the Outer Rim.\"\n",
    "    }, {\n",
    "        \"question\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {question}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix for user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# Select examples from the examples list without exceeding the context window\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt_template,\n",
    "    max_length=50  # this sets the max length that examples should be. Measured in words.\n",
    ")\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    # examples=examples, # This will load all the examples, which may exceed to context window\n",
    "    example_prompt=example_prompt_template,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is the meaning of life?\"\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the output\n",
    "print(openai_llm(\n",
    "    few_shot_prompt_template.format(query=query)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Working with Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llm (non-chat)\n",
    "from langchain.chat_models import ChatOpenAI # For chat models\n",
    "\n",
    "#Other model parameters\n",
    "temperature = 0.5\n",
    "\n",
    "# Chat model eg \"gpt-3.5-turbo\"\n",
    "chat_model_name=\"gpt-3.5-turbo\"\n",
    "openai_chat_llm = ChatOpenAI(\n",
    "    model_name=chat_model_name,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare messages for the chat\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpfull assistant\"),\n",
    "    HumanMessage(content=\"How are you today?\"),\n",
    "    AIMessage(content=\"I'm great. How can I help?\"),\n",
    "    HumanMessage(content=\"I'd like to understand women?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a response\n",
    "res = openai_chat_llm(messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append response, ask next question and get next response\n",
    "messages.append(res)\n",
    "next_prompt=HumanMessage(content=\"How do I get a random woman's attention?\")\n",
    "messages.append(next_prompt)\n",
    "res = openai_chat_llm(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates for chats\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    '{input}. Can you keep the response less than 140 characters'\n",
    ")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([human_template])\n",
    "chat_prompt = chat_template.format_prompt(input=\"What should I do today?\")\n",
    "chat_prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "chat_prompt.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same for other types of messages\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    'You are a helpful assistant. You keep responses to no more than '\n",
    "    '{character_limit} characters long (including whitespace), and sign '\n",
    "    'off every message with \"- {sign_off}'\n",
    ")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "ai_template = AIMessagePromptTemplate.from_template(\"{response} - {sign_off}\")\n",
    "\n",
    "# create the list of messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_template,\n",
    "    human_template,\n",
    "    ai_template\n",
    "])\n",
    "# format with required inputs\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    character_limit=\"50\", sign_off=\"Robot McRobot\",\n",
    "    input=\"Hi AI, how are you? What is quantum physics?\",\n",
    "    response=\"Good! It's physics of small things\"\n",
    ")\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract these as messages and feed them into the chat model alongside our next query\n",
    "messages = chat_prompt_value.to_messages()\n",
    "\n",
    "messages.append(\n",
    "    HumanMessage(content=\"How small?\")\n",
    ")\n",
    "\n",
    "res = openai_chat_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
